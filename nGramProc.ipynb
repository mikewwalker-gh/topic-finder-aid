{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGvzBHGBq9kKXeRH4BdgiU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikewwalker-gh/topic-finder-aid/blob/main/nGramProc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import re\n"
      ],
      "metadata": {
        "id": "eAplU5tXjD4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read  all or read a sample of  random text files from directory path into a df\n",
        "directory_path = '/opt/app/agent-assist/data/chat/transcripts/2022-10'\n",
        "df = read_text_files_to_df(directory_path)  # Read all files into a DataFrame\n",
        "# df = read_text_files_to_df(directory_path, num_files_to_sample=100)  # Read a random sample of 10,000 files into a DataFrame"
      ],
      "metadata": {
        "id": "EqXsWiiejEvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess df['topic']"
      ],
      "metadata": {
        "id": "7MMMkDvHjJxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another column in df, df'['clean_topic'] and call function clean_column() to clean repetitive, noisy strings.\n",
        "# df['clean_chat'] = clean_column(df, 'chat')\n",
        "df['clean_topic'] = clean_column(df, 'topic')"
      ],
      "metadata": {
        "id": "7kG5r9COjP3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another step in cleaning, remove strings matching regex patterns\n",
        "df['clean_topic'] = apply_regex_replacements(df, 'clean_topic', regex_patterns)"
      ],
      "metadata": {
        "id": "xhOkipodjT_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation from df['clean_topic']\n",
        "df['clean_topic'] = df['clean_topic'].str.replace('[^\\w\\s]', '')"
      ],
      "metadata": {
        "id": "1TxNBi81jYHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove occurance of one or more spaces with a single space in df['clean_topic']\n",
        "df['clean_topic'] = df['clean_topic'].str.replace(' +', ' ')"
      ],
      "metadata": {
        "id": "GxqgWgtVjcuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation and digits, split the text into words, filter out non-alpha words and rejoin the words.\n",
        "df['clean_topic'] = df['clean_topic'].apply(clean_text)"
      ],
      "metadata": {
        "id": "j41hmwNnjl_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare size of all content in all rows of column 'topic' in df to size of all text in column 'clean_topic' in df\n",
        "\n",
        "print(df['topic'].str.len().sum())\n",
        "print(df['clean_topic'].str.len().sum())"
      ],
      "metadata": {
        "id": "5nzkMip7jqxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "bALulmpVjwAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and fileter for stop words, also convert to lower case before processing.\n",
        "\n",
        "df['topic_tokens'] = df['clean_topic'].apply(tokenize_and_filter)"
      ],
      "metadata": {
        "id": "Uj0MXkh5j0O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add ngrams in columns - unigrams, bigrams, trigrams and quadgrams to the df.\n",
        "df = add_ngram_columns(df)"
      ],
      "metadata": {
        "id": "oD3zYIojj3yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lu_mHBuFQv5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read df here, and them lemmatize 'clean_topic' and all ngram columns; Also\n",
        "# remove the max_rows_per_ngram restriction when calling create_training_data\n",
        "# function below. %% Assuming df has 'unigrams', 'bigrams', 'trigrams',\n",
        "# 'quadgrams' columns"
      ],
      "metadata": {
        "id": "K5KmoRU5QMT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df has 'unigrams', 'bigrams', 'trigrams', 'quadgrams' columns\n",
        "freq_dists = compute_ngram_freq_dist(df, 'unigrams', 'bigrams', 'trigrams', 'quadgrams')"
      ],
      "metadata": {
        "id": "m1PmCuIqj7Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data using df, freq_dists and keywords from taxonomy:\n",
        "# df = your dataframe with columns: 'filename', 'topic', 'unigrams', 'bigrams', 'trigrams', 'quadgrams'\n",
        "# freq_dists = your frequency distributions for 'unigrams', 'bigrams', 'trigrams', 'quadgrams'\n",
        "keywords = ['claim', 'coverage','enrollment','medicare','medicaregov','premium','social security']\n",
        "training_df = create_training_data(df, freq_dists, keywords, top_n=500, max_rows_per_ngram=25)\n",
        "# print(training_df)"
      ],
      "metadata": {
        "id": "Xce83ETQj-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word, frequency in freq_dists['unigrams'].most_common(20):\n",
        "#     print(word, frequency)\n",
        "# len(freq_dists['quadgrams'])\n",
        "freq_dists['trigrams'].most_common(5)\n"
      ],
      "metadata": {
        "id": "noMHY-HrkCT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_rows', None)  # Display all rows\n",
        "# training_df.sort_values(by='Keyword',ascending=True)\n",
        "training_df.to_csv('training_df_500_25b')"
      ],
      "metadata": {
        "id": "8NzWJE2gkGPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################## Run Functions and set variables; do this first ##############################\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "Fptl8oYvkOHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex_patterns = [\n",
        "    r'My name is \\[.*?\\]',  # Pattern to match 'My name is [Agent Name]'\n",
        "    r'Party \\[.*?\\] left the session',  # Pattern to match 'Party [Agent Name] left the session'\n",
        "    r'\\[.*?\\] has joined the chat',  # Pattern to match '[Agent Name] has joined the chat'\n",
        "    r'you have a nice (day|night|afternoon)',  # Pattern to match 'day, night, afternoon'\n",
        "    r'Please do not give me (his|her) Medicare number',\n",
        "    r'\\[.*?\\] has left the chat',  # Pattern to match '[Agent Name] has left the chat'\n",
        "    r'my name is ([a-zA-Z]+(?: [a-zA-Z]+)*)\\.',  # Pattern to match 'My name is [Name].'\n",
        "    r'\\[.*?\\] is typing',  # Pattern to match '[Agent Name] is typing'\n",
        "    r\"thank you for your help \\w+\",  # Pattern to match 'thank you for your help [Agent Name]'\n",
        "\n",
        "\n",
        "\n",
        "    # ...\n",
        "]"
      ],
      "metadata": {
        "id": "P2YzkXTWkYBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_column(df, column_name):\n",
        "    # Define static strings to remove\n",
        "    strings_to_remove = {\n",
        "        \"Please wait while you are connected to an agent\",\n",
        "        \"You are now connected with Medicare.gov Live Chat\",\n",
        "        \"Can you please verify your first and last name?\",\n",
        "        \"User is Typing\",\n",
        "        \"Agent is Typing\",\n",
        "        \"Thank you\",\n",
        "        \"sir\",\n",
        "        \"The chat session is being wrapped up\",\n",
        "        \"Thank you for contacting Medicare.gov Live Chat\",\n",
        "        \"for contacting Medicare.gov Live Chat\",\n",
        "        \"You are now connected with Medicare.gov Live Chat Thank you for contacting Medicare.gov Live Chat\",\n",
        "        \"We are here to help you 24 hours a day, 7 days a week\",\n",
        "        \"Left the session\",\n",
        "        \"The chat session has ended\",\n",
        "        \"All times in the above transcript are in the following time zone: (GMT-05:00) Indiana (East)\",\n",
        "        \"It appears that you can’t see my reply, so I’m going to end the chat\",\n",
        "        \"If you can see this message and you still need assistance\",\n",
        "        \"please initiate a new chat or call 1-800-MEDICARE (1-800-633-4227)\",\n",
        "        \"Representatives are available to assist you 24 hours a day, 7 days a week\",\n",
        "        \"My pleasure\",\n",
        "        # \"Thank you\",\n",
        "        \"you too\",\n",
        "        # \"thank you\",\n",
        "        \"For privacy purposes, please do not disclose any personal information such as your Social Security Number, Medicare ID, or any other sensitive medical or personal information\",\n",
        "        \"are you still there?\",\n",
        "        \"I am still researching that information\",\n",
        "        \"It will take me just a moment to review and respond to your question\",\n",
        "        \"Thank you for your patience\",\n",
        "        \"One moment please while I look that up\",\n",
        "        \"One moment\",\n",
        "        \"Hello\",\n",
        "        \"Hi\",\n",
        "        \"TTY users can call 1-877-486-2048\",\n",
        "        \"I'll be happy to assist you today\"\n",
        "        # Add more static replacements as needed\n",
        "    }\n",
        "\n",
        "    # Check if the column exists in the dataframe\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"The column {column_name} does not exist in the dataframe.\")\n",
        "\n",
        "    # Copy the column to avoid modifying the original data\n",
        "    cleaned_column = df[column_name].copy()\n",
        "\n",
        "    # Replace /n and <br> with ' '\n",
        "    cleaned_column = cleaned_column.str.replace(\"\\n\", \"\", regex=False)\n",
        "    cleaned_column = cleaned_column.str.replace(r\"<br>\", ' ', regex=True)\n",
        "\n",
        "    # Remove static strings\n",
        "    for string in strings_to_remove:\n",
        "        cleaned_column = cleaned_column.str.replace(string, '', regex=False)\n",
        "\n",
        "    # Return the cleaned column\n",
        "    return cleaned_column\n",
        "\n",
        "# Usage:\n",
        "# df['clean_chat'] = clean_column(df, 'Chat')\n",
        "# df['clean_topic'] = clean_column(df, 'Topic')\n"
      ],
      "metadata": {
        "id": "d8udfhpFkZfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def apply_regex_replacements(df, column_name, regex_patterns):\n",
        "    # Check if the column exists in the dataframe\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"The column {column_name} does not exist in the dataframe.\")\n",
        "\n",
        "    # Copy the column to avoid modifying the original data\n",
        "    cleaned_column = df[column_name].copy()\n",
        "\n",
        "    # Apply regex pattern replacements\n",
        "    for pattern in regex_patterns:\n",
        "        cleaned_column = cleaned_column.str.replace(pattern, '', regex=True)\n",
        "\n",
        "    # Return the cleaned column\n",
        "    return cleaned_column\n",
        "\n",
        "# Usage:\n",
        "# regex_patterns = [\n",
        "#     r'My name is \\[.*?\\]',  # Pattern to match 'My name is [Agent Name]'\n",
        "#     # ... (include all other patterns to remove here)\n",
        "#     r\"thank you for your help \\w+\"  # Pattern to match 'thank you for your help [Agent Name]'\n",
        "# ]\n",
        "# df['clean_chat'] = apply_regex_replacements(df, 'clean_chatclean', regex_patterns)\n"
      ],
      "metadata": {
        "id": "5HbL-VJSkc73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove all punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes punctuation\n",
        "    text = re.sub(r'\\d', '', text)       # Removes digits\n",
        "    # Split the text into words, filter out non-alphabetic words, and rejoin the words\n",
        "    cleaned = ' '.join(w for w in text.split() if w.isalpha())\n",
        "    return cleaned\n",
        "\n",
        "# Usage\n",
        "# df['clean_chat'] = df['clean_chat'].apply(clean_text)\n",
        "# df['clean_topic'] = df['clean_topic'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "_EogsZ_Vkg4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def read_text_files_to_df(directory_path, num_files_to_sample=None):\n",
        "    # Get a list of all text files (with .txt extension) in the directory\n",
        "    all_files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.txt')]\n",
        "\n",
        "    # Select files\n",
        "    if num_files_to_sample is not None:\n",
        "        num_files_to_select = min(num_files_to_sample, len(all_files))\n",
        "        selected_files = random.sample(all_files, num_files_to_select)\n",
        "    else:\n",
        "        selected_files = all_files  # Use all files if no specific sample size is provided\n",
        "\n",
        "    # Read the selected files and store filename, topic, and chat content\n",
        "    data = []\n",
        "    for file in selected_files:\n",
        "        with open(os.path.join(directory_path, file), 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            topic = lines[0].strip()  # First line is the topic\n",
        "            chat = ' '.join(lines[1:]).strip()  # Rest of the lines are the chat\n",
        "            filename = os.path.splitext(file)[0]  # Filename without extension\n",
        "            data.append({'filename': filename, 'topic': topic, 'chat': chat})\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    df_all = pd.DataFrame(data)\n",
        "    return df_all\n",
        "\n",
        "# Usage:\n",
        "# directory_path = '/opt/app/agent-assist/data/chat/transcripts/2022-10'\n",
        "# df_all = read_text_files_to_df(directory_path)  # Read all files into a DataFrame\n",
        "# df_sample = read_text_files_to_df(directory_path, num_files_to_sample=10000)  # Read a sample of 10,000 files into a DataFrame\n"
      ],
      "metadata": {
        "id": "_lLGky1Fklb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and fileter for stop words, also convert to lower case before processing.\n",
        "def tokenize_and_filter(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # return tokens\n",
        "    return [w for w in tokens if w not in stop_words]"
      ],
      "metadata": {
        "id": "CJ1XI1ZIksQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def add_ngram_columns(df):\n",
        "    # Define a function to generate n-grams from tokens\n",
        "    def generate_ngrams(tokens, n):\n",
        "        return [' '.join(ngram) for ngram in ngrams(tokens, n)]\n",
        "\n",
        "    # Check if 'topic_tokens' column exists\n",
        "    if 'topic_tokens' not in df.columns:\n",
        "        raise ValueError(\"The dataframe does not contain a 'topic_tokens' column.\")\n",
        "\n",
        "    # Apply the generate_ngrams function to each row in the DataFrame for each n value\n",
        "    df['unigrams'] = df['topic_tokens'].apply(lambda tokens: generate_ngrams(tokens, 1))\n",
        "    df['bigrams'] = df['topic_tokens'].apply(lambda tokens: generate_ngrams(tokens, 2))\n",
        "    df['trigrams'] = df['topic_tokens'].apply(lambda tokens: generate_ngrams(tokens, 3))\n",
        "    df['quadgrams'] = df['topic_tokens'].apply(lambda tokens: generate_ngrams(tokens, 4))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage:\n",
        "# df = add_ngram_columns(df)\n"
      ],
      "metadata": {
        "id": "pGx2NM5ClA-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk import FreqDist\n",
        "\n",
        "def compute_ngram_freq_dist(df, *columns):\n",
        "    freq_dists = {}\n",
        "    for column in columns:\n",
        "        # Check if column exists\n",
        "        if column not in df.columns:\n",
        "            raise ValueError(f\"The dataframe does not contain a '{column}' column.\")\n",
        "\n",
        "        # Flatten the list of lists to a single list of n-grams\n",
        "        ngrams_list = [ngram for sublist in df[column] for ngram in sublist]\n",
        "\n",
        "        # Compute the frequency distribution of the n-grams\n",
        "        freq_dist = FreqDist(ngrams_list)\n",
        "        freq_dists[column] = freq_dist\n",
        "\n",
        "    return freq_dists\n",
        "\n",
        "# Usage:\n",
        "# Assuming df has 'unigrams', 'bigrams', 'trigrams', 'quadgrams' columns\n",
        "# freq_dists = compute_ngram_freq_dist(df, 'unigrams', 'bigrams', 'trigrams', 'quadgrams')\n"
      ],
      "metadata": {
        "id": "3ZyWLWTslEnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure display options\n",
        "# pd.set_option('display.max_colwidth', None)  # Display full content of each column\n",
        "# pd.set_option('display.max_rows', None)  # Display all rows"
      ],
      "metadata": {
        "id": "2OL1TjgRlJDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_training_data(df, freq_dists, keywords, top_n, max_rows_per_ngram):\n",
        "    # Initialize list to hold the data\n",
        "    training_data = []\n",
        "\n",
        "    # Iterate through each ngram type\n",
        "    for ngram_type in ['unigrams', 'bigrams', 'trigrams', 'quadgrams']:\n",
        "        # Get the top_n ngrams and their frequencies\n",
        "        ngrams_freqs = freq_dists[ngram_type].most_common(top_n)\n",
        "\n",
        "        # Iterate through each keyword\n",
        "        for keyword in keywords:\n",
        "            # Filter ngrams that contain the keyword exclusively\n",
        "            filtered_ngrams = [ngram for ngram, freq in ngrams_freqs if keyword in ngram and all(other_keyword not in ngram for other_keyword in keywords if other_keyword != keyword)]\n",
        "\n",
        "            # For each filtered ngram, find the rows in df where this ngram appears\n",
        "            for ngram in filtered_ngrams:\n",
        "                ngram_str = ' '.join(ngram)\n",
        "                matched_rows = df[df[ngram_type].apply(lambda x: ngram_str in [' '.join(y) for y in x])]\n",
        "\n",
        "                # Limit the matched rows to max_rows_per_ngram\n",
        "                limited_matched_rows = matched_rows.head(max_rows_per_ngram)\n",
        "\n",
        "                # For each matched row, store the keyword, ngram, and topic\n",
        "                for _, row in limited_matched_rows.iterrows():\n",
        "                    training_data.append({\n",
        "                        'Keyword': keyword,\n",
        "                        'Ngram': ngram_str,\n",
        "                        'Topic': row['topic'],\n",
        "                        'Filename': row['filename']  # Assuming there's a 'filename' column\n",
        "                    })\n",
        "\n",
        "    # Convert the training data into a DataFrame\n",
        "    training_df = pd.DataFrame(training_data, columns=['Keyword', 'Ngram', 'Topic', 'Filename'])\n",
        "    training_df['Ngram'] = training_df['Ngram'].str.replace(r'(?<=\\b\\w) (?=\\w\\b)', '', regex=True).str.replace(r'\\s{2,}', ' ', regex=True)\n",
        "\n",
        "    return training_df\n",
        "\n",
        "# Usage:\n",
        "# df = your dataframe with columns: 'filename', 'topic', 'unigrams', 'bigrams', 'trigrams', 'quadgrams'\n",
        "# freq_dists = your frequency distributions for 'unigrams', 'bigrams', 'trigrams', 'quadgrams'\n",
        "# keywords = ['claim', 'another_keyword', ...]\n",
        "# training_df = create_training_data(df, freq_dists, keywords, top_n=1000, max_rows_per_ngram=10)\n",
        "# print(training_df)\n"
      ],
      "metadata": {
        "id": "SRRLzSJllN6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxH2bXJklSnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}